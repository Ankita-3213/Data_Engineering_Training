{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Module 1: Setup & SparkSession Initialization"
      ],
      "metadata": {
        "id": "uXK9oIXAjV3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Edd244sMyZQ"
      },
      "outputs": [],
      "source": [
        "# Creating Spark Session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"BotCampus PySpark Practice\").master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataFrame\n",
        "data = [\n",
        "(\"Anjali\", \"Bangalore\", 24),\n",
        "(\"Ravi\", \"Hyderabad\", 28),\n",
        "(\"Kavya\", \"Delhi\", 22),\n",
        "(\"Meena\", \"Chennai\", 25),\n",
        "(\"Arjun\", \"Mumbai\", 30)\n",
        "]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df = spark.createDataFrame(data,columns)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "82A2naLNj0ks",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ff532a-2e30-449b-d979-fdcf0288761b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Anjali|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "| Arjun|   Mumbai| 30|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Schema\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBbUgqB-ayEm",
        "outputId": "2d3329e4-244e-45d0-836b-99e18a4d7898"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Datatypes\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM-X1xBEbFiQ",
        "outputId": "9837dd8a-6f28-445f-9871-d9b785515b44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('name', 'string'), ('city', 'string'), ('age', 'bigint')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to DataFrame\n",
        "rdd = df.rdd\n",
        "print(rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNJV38fEbYf8",
        "outputId": "1e10a643-e2fa-491d-9afb-ccf0f2f62db0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(name='Anjali', city='Bangalore', age=24), Row(name='Ravi', city='Hyderabad', age=28), Row(name='Kavya', city='Delhi', age=22), Row(name='Meena', city='Chennai', age=25), Row(name='Arjun', city='Mumbai', age=30)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapped_rdd = df.rdd.map(lambda row: (row.name.upper(), row.age))\n",
        "print(\"df.rdd.map() output:\")\n",
        "for item in mapped_rdd.collect():\n",
        "    print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inxeJBx7cOZr",
        "outputId": "140dd2a6-2d9a-4730-faef-75a10aab615f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df.rdd.map() output:\n",
            "('ANJALI', 24)\n",
            "('RAVI', 28)\n",
            "('KAVYA', 22)\n",
            "('MEENA', 25)\n",
            "('ARJUN', 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Module 2: RDDs & Transformations"
      ],
      "metadata": {
        "id": "5sB7wDdedvja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedback RDD\n",
        "feedback = spark.sparkContext.parallelize([\n",
        "\"Ravi from Bangalore loved the delivery\",\n",
        "\"Meena from Hyderabad had a late order\",\n",
        "\"Ajay from Pune liked the service\",\n",
        "\"Anjali from Delhi faced UI issues\",\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])"
      ],
      "metadata": {
        "id": "Mmf_Rc0QdspP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split each line into words (flatMap )\n",
        "split_words = (feedback.flatMap(lambda line: line.split()))\n",
        "split_words.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7MWOtOresZv",
        "outputId": "4e6b14f6-1e48-4938-8b74-5b139c5c6805"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ravi',\n",
              " 'from',\n",
              " 'Bangalore',\n",
              " 'loved',\n",
              " 'the',\n",
              " 'delivery',\n",
              " 'Meena',\n",
              " 'from',\n",
              " 'Hyderabad',\n",
              " 'had',\n",
              " 'a',\n",
              " 'late',\n",
              " 'order',\n",
              " 'Ajay',\n",
              " 'from',\n",
              " 'Pune',\n",
              " 'liked',\n",
              " 'the',\n",
              " 'service',\n",
              " 'Anjali',\n",
              " 'from',\n",
              " 'Delhi',\n",
              " 'faced',\n",
              " 'UI',\n",
              " 'issues',\n",
              " 'Rohit',\n",
              " 'from',\n",
              " 'Mumbai',\n",
              " 'gave',\n",
              " 'positive',\n",
              " 'feedback']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stop Words\n",
        "# Creating a list of stop words\n",
        "stop_words = {\"from\", \"is\", \"with\", \"the\", \"a\",\"an\", \"of\", \"and\", \"on\", \"to\", \"in\" }\n",
        "no_stop_words = (feedback.flatMap(lambda line: line.split())\n",
        "                .map(lambda word : word.lower())\n",
        "                .filter(lambda word: word not in stop_words))\n",
        "no_stop_words.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyI8ITmHitcs",
        "outputId": "4794d53c-cdfb-46f9-9bc3-a1d3f13efe73"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ravi',\n",
              " 'bangalore',\n",
              " 'loved',\n",
              " 'delivery',\n",
              " 'meena',\n",
              " 'hyderabad',\n",
              " 'had',\n",
              " 'late',\n",
              " 'order',\n",
              " 'ajay',\n",
              " 'pune',\n",
              " 'liked',\n",
              " 'service',\n",
              " 'anjali',\n",
              " 'delhi',\n",
              " 'faced',\n",
              " 'ui',\n",
              " 'issues',\n",
              " 'rohit',\n",
              " 'mumbai',\n",
              " 'gave',\n",
              " 'positive',\n",
              " 'feedback']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count each word frequency using ReduceByKey\n",
        "word_count =(feedback.flatMap(lambda feedback: feedback.split())\n",
        "             .map(lambda w: (w.lower(), 1))\n",
        "             .reduceByKey(lambda a,b: a+b))\n",
        "word_count.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gBJ_oR4i8HV",
        "outputId": "069c354a-dd7e-4d73-b5f4-1a099201c3b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('from', 5),\n",
              " ('loved', 1),\n",
              " ('liked', 1),\n",
              " ('service', 1),\n",
              " ('anjali', 1),\n",
              " ('faced', 1),\n",
              " ('issues', 1),\n",
              " ('rohit', 1),\n",
              " ('mumbai', 1),\n",
              " ('positive', 1),\n",
              " ('feedback', 1),\n",
              " ('ravi', 1),\n",
              " ('bangalore', 1),\n",
              " ('the', 2),\n",
              " ('delivery', 1),\n",
              " ('meena', 1),\n",
              " ('hyderabad', 1),\n",
              " ('had', 1),\n",
              " ('a', 1),\n",
              " ('late', 1),\n",
              " ('order', 1),\n",
              " ('ajay', 1),\n",
              " ('pune', 1),\n",
              " ('delhi', 1),\n",
              " ('ui', 1),\n",
              " ('gave', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find top 3 most frequent non-stop words\n",
        "counts = no_stop_words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "top_3 = counts.takeOrdered(3, key=lambda x: -x[1])\n",
        "print(top_3)"
      ],
      "metadata": {
        "id": "AAMn5QAwjBn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26abd1fe-090c-4e1c-ae20-7dacb341bca3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('loved', 1), ('liked', 1), ('service', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module 3: DataFrames & Transformation (With Joins)\n"
      ],
      "metadata": {
        "id": "yqFIvadQh-kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating DataFrames students and attendance\n",
        "students = [\n",
        "(\"Amit\", \"10-A\", 89),\n",
        "(\"Kavya\", \"10-B\", 92),\n",
        "(\"Anjali\", \"10-A\", 78),\n",
        "(\"Rohit\", \"10-B\", 85),\n",
        "(\"Sneha\", \"10-C\", 80)\n",
        "]\n",
        "columns = [\"name\", \"section\", \"marks\"]\n",
        "\n",
        "attendance = [\n",
        "(\"Amit\", 24),\n",
        "(\"Kavya\", 22),\n",
        "(\"Anjali\", 20),\n",
        "(\"Rohit\", 25),\n",
        "(\"Sneha\", 19)\n",
        "]\n",
        "columns2 = [\"name\", \"days_present\"]"
      ],
      "metadata": {
        "id": "o3vm2Yjqhenn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_students = spark.createDataFrame(students, columns)\n",
        "df_students.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HElek5oSZL",
        "outputId": "5635c2c1-7b64-4afd-ac45-67791fde2f22"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+\n",
            "|  name|section|marks|\n",
            "+------+-------+-----+\n",
            "|  Amit|   10-A|   89|\n",
            "| Kavya|   10-B|   92|\n",
            "|Anjali|   10-A|   78|\n",
            "| Rohit|   10-B|   85|\n",
            "| Sneha|   10-C|   80|\n",
            "+------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_attendance = spark.createDataFrame(attendance, columns2)\n",
        "df_attendance.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYIMb-j8opAR",
        "outputId": "4e740eae-b6f1-4648-fae8-366af272e243"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+\n",
            "|  name|days_present|\n",
            "+------+------------+\n",
            "|  Amit|          24|\n",
            "| Kavya|          22|\n",
            "|Anjali|          20|\n",
            "| Rohit|          25|\n",
            "| Sneha|          19|\n",
            "+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join both DataFrames on name\n",
        "df_info = df_students.join(df_attendance, on=\"name\", how=\"inner\")\n",
        "df_info.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWsr3BSajq2N",
        "outputId": "396b7505-be70-4b7b-cf7f-af26987cf7ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+\n",
            "|  name|section|marks|days_present|\n",
            "+------+-------+-----+------------+\n",
            "|  Amit|   10-A|   89|          24|\n",
            "|Anjali|   10-A|   78|          20|\n",
            "| Kavya|   10-B|   92|          22|\n",
            "| Rohit|   10-B|   85|          25|\n",
            "| Sneha|   10-C|   80|          19|\n",
            "+------+-------+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new column: attendance_rate = days_present / 25\n",
        "from pyspark.sql.functions import col\n",
        "df_attendance_rate = df_info.withColumn(\"attendance_rate\", col(\"days_present\")/25)\n",
        "df_attendance_rate.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNl-oueyjqif",
        "outputId": "e22c9bf6-c054-470d-9759-10de0d33c175"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+---------------+\n",
            "|  name|section|marks|days_present|attendance_rate|\n",
            "+------+-------+-----+------------+---------------+\n",
            "|  Amit|   10-A|   89|          24|           0.96|\n",
            "|Anjali|   10-A|   78|          20|            0.8|\n",
            "| Kavya|   10-B|   92|          22|           0.88|\n",
            "| Rohit|   10-B|   85|          25|            1.0|\n",
            "| Sneha|   10-C|   80|          19|           0.76|\n",
            "+------+-------+-----+------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Grade students using when and otherwise A: >90, B: 80–90, C: <80.\n",
        "from pyspark.sql.functions import when\n",
        "df_grade = df_info.withColumn(\"grade\",\n",
        "                              when(col(\"marks\") > 90, \"A\")\n",
        "                            .when((col(\"marks\") >= 80 ) & (col(\"marks\")<=90),\"B\")\n",
        "                            .otherwise(\"C\"))\n",
        "\n",
        "df_grade.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHOmfDEGjqHN",
        "outputId": "9b3556c2-9264-4f0a-edc3-1450f2b74608"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+-----+\n",
            "|  name|section|marks|days_present|grade|\n",
            "+------+-------+-----+------------+-----+\n",
            "|  Amit|   10-A|   89|          24|    B|\n",
            "|Anjali|   10-A|   78|          20|    C|\n",
            "| Kavya|   10-B|   92|          22|    A|\n",
            "| Rohit|   10-B|   85|          25|    B|\n",
            "| Sneha|   10-C|   80|          19|    B|\n",
            "+------+-------+-----+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter students with good grades but poor attendance (<80%)\n",
        "\n",
        "df_final = df_grade.join(df_attendance_rate.select(\"name\", \"attendance_rate\"), on=\"name\", how=\"inner\")\n",
        "df_filtered = df_final.filter(\n",
        "    ((col(\"grade\") == \"A\") | (col(\"grade\") == \"B\")) & (col(\"attendance_rate\") < 0.8)\n",
        ")\n",
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCuPq_zrjp0j",
        "outputId": "0971d584-0f0b-45be-b61f-5708559561a1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+------------+-----+---------------+\n",
            "| name|section|marks|days_present|grade|attendance_rate|\n",
            "+-----+-------+-----+------------+-----+---------------+\n",
            "|Sneha|   10-C|   80|          19|    B|           0.76|\n",
            "+-----+-------+-----+------------+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYemGeHAvkuU",
        "outputId": "8b02560e-6a09-458b-8396-356f01a5c6a2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+-----+---------------+\n",
            "|  name|section|marks|days_present|grade|attendance_rate|\n",
            "+------+-------+-----+------------+-----+---------------+\n",
            "|  Amit|   10-A|   89|          24|    B|           0.96|\n",
            "|Anjali|   10-A|   78|          20|    C|            0.8|\n",
            "| Kavya|   10-B|   92|          22|    A|           0.88|\n",
            "| Rohit|   10-B|   85|          25|    B|            1.0|\n",
            "| Sneha|   10-C|   80|          19|    B|           0.76|\n",
            "+------+-------+-----+------------+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module 4: Ingest CSV & JSON, Save to Parquet"
      ],
      "metadata": {
        "id": "Sv9_k05NHXNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ingesting csv and json\n",
        "data_1 = \"\"\"\n",
        " emp_id,name,dept,city,salary\n",
        "101,Anil,IT,Bangalore,80000\n",
        "102,Kiran,HR,Mumbai,65000\n",
        "103,Deepa,Finance,Chennai,72000\n",
        "\"\"\"\n",
        "\n",
        "with open('employees.csv', 'w') as file:\n",
        "  file.write(data_1)\n",
        "\n"
      ],
      "metadata": {
        "id": "81rKFWStHf7J"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "data_2 = {\n",
        "\n",
        " \"id\": 201,\n",
        "\"name\": \"Nandini\",\n",
        "\"contact\": {\n",
        "\"email\": \"nandi@example.com\",\n",
        "\"city\": \"Hyderabad\"\n",
        "},\n",
        "\"skills\": [\"Python\", \"Spark\", \"SQL\"]\n",
        " }\n",
        "\n",
        "with open('employees.json', 'w') as file:\n",
        "  json.dump(data_2, file, indent=4)\n"
      ],
      "metadata": {
        "id": "3Os8kTq6gyHg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Read both formats into DataFrames\n",
        "df_csv = spark.read.csv('employees.csv', header=True, inferSchema=True)\n",
        "\n",
        "df_csv.show()\n",
        "\n",
        "df_json = spark.read.option(\"multiline\", True).json('employees.json')\n",
        "df_json.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-0tLHC9hxKC",
        "outputId": "a3cffbde-9857-4235-da6c-dbe1f135483a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-------+---------+------+\n",
            "| emp_id| name|   dept|     city|salary|\n",
            "+-------+-----+-------+---------+------+\n",
            "|    101| Anil|     IT|Bangalore| 80000|\n",
            "|    102|Kiran|     HR|   Mumbai| 65000|\n",
            "|    103|Deepa|Finance|  Chennai| 72000|\n",
            "+-------+-----+-------+---------+------+\n",
            "\n",
            "+--------------------+---+-------+--------------------+\n",
            "|             contact| id|   name|              skills|\n",
            "+--------------------+---+-------+--------------------+\n",
            "|{Hyderabad, nandi...|201|Nandini|[Python, Spark, SQL]|\n",
            "+--------------------+---+-------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten nested JSON using select ,col ,alias ,explode\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "flattened = df_json.select(\n",
        "    col('id'),\n",
        "    col('name'),\n",
        "    col('contact.email').alias('email'),\n",
        "    col('contact.city').alias('city'),\n",
        "    explode(col('skills')).alias('skills')\n",
        ")\n",
        "\n",
        "flattened.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGFkECMLiLJn",
        "outputId": "904aacad-b858-4257-fe6f-dbc9540355bd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-----------------+---------+------+\n",
            "| id|   name|            email|     city|skills|\n",
            "+---+-------+-----------------+---------+------+\n",
            "|201|Nandini|nandi@example.com|Hyderabad|Python|\n",
            "|201|Nandini|nandi@example.com|Hyderabad| Spark|\n",
            "|201|Nandini|nandi@example.com|Hyderabad|   SQL|\n",
            "+---+-------+-----------------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save both as Parquet files partitioned by city\n",
        "df_csv.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"output/employees_csv\")\n",
        "flattened.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"output/employees_json\")\n"
      ],
      "metadata": {
        "id": "DK2qp2Alikvb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module 5: Spark SQL with Temp Views\n"
      ],
      "metadata": {
        "id": "XpCA3ZkYiqK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_final.show()"
      ],
      "metadata": {
        "id": "TGNx0yAmixPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e538042-6e69-4e8a-81a3-d25e12e313d5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+------------+-----+---------------+\n",
            "|  name|section|marks|days_present|grade|attendance_rate|\n",
            "+------+-------+-----+------------+-----+---------------+\n",
            "|  Amit|   10-A|   89|          24|    B|           0.96|\n",
            "|Anjali|   10-A|   78|          20|    C|            0.8|\n",
            "| Kavya|   10-B|   92|          22|    A|           0.88|\n",
            "| Rohit|   10-B|   85|          25|    B|            1.0|\n",
            "| Sneha|   10-C|   80|          19|    B|           0.76|\n",
            "+------+-------+-----+------------+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the students DataFrame as students_view .\n",
        "df_final.createOrReplaceTempView('students_view')"
      ],
      "metadata": {
        "id": "sKeRdYvkv3He"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average marks per section\n",
        "spark.sql(\"select section, avg(marks) from students_view group by section\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IGY5EnOwAlX",
        "outputId": "7a562861-f089-4464-c478-2ff0c7182f18"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|section|avg(marks)|\n",
            "+-------+----------+\n",
            "|   10-C|      80.0|\n",
            "|   10-A|      83.5|\n",
            "|   10-B|      88.5|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top scorer in each section\n",
        "spark.sql(\"select section, name, marks from (select *, ROW_NUMBER() over(PARTITION BY section ORDER BY marks desc)as rnk from students_view) where rnk =1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-XaCVeKwMxz",
        "outputId": "ddf9a070-4b02-416f-a14b-de8514ce75e7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+-----+\n",
            "|section| name|marks|\n",
            "+-------+-----+-----+\n",
            "|   10-A| Amit|   89|\n",
            "|   10-B|Kavya|   92|\n",
            "|   10-C|Sneha|   80|\n",
            "+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of students in each grade category\n",
        "spark.sql(\"select grade, count(*) from students_view group by grade\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UsPSDfUwQVx",
        "outputId": "7e53cd4d-b63e-4b15-810d-e4baeb2c07ce"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+\n",
            "|grade|count(1)|\n",
            "+-----+--------+\n",
            "|    B|       3|\n",
            "|    C|       1|\n",
            "|    A|       1|\n",
            "+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Students with marks above class average\n",
        "spark.sql(\"select * from students_view where marks > (select avg(marks) from students_view)\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsFy7bTDwUax",
        "outputId": "10398984-e1e3-4304-cd46-ee0f531d9211"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+-----+------------+-----+---------------+\n",
            "| name|section|marks|days_present|grade|attendance_rate|\n",
            "+-----+-------+-----+------------+-----+---------------+\n",
            "| Amit|   10-A|   89|          24|    B|           0.96|\n",
            "|Kavya|   10-B|   92|          22|    A|           0.88|\n",
            "|Rohit|   10-B|   85|          25|    B|            1.0|\n",
            "+-----+-------+-----+------------+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attendance-adjusted performance\n",
        "spark.sql(\"\"\"\n",
        "    SELECT\n",
        "        name,\n",
        "        section,\n",
        "        marks,\n",
        "        attendance_rate,\n",
        "        ROUND(marks * (attendance_rate / 100), 2) AS adjusted_performance\n",
        "    FROM students_view\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbTL0bp9wX31",
        "outputId": "57436b0b-a809-4b76-cb52-72e6149e55d9"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+---------------+--------------------+\n",
            "|  name|section|marks|attendance_rate|adjusted_performance|\n",
            "+------+-------+-----+---------------+--------------------+\n",
            "|  Amit|   10-A|   89|           0.96|                0.85|\n",
            "|Anjali|   10-A|   78|            0.8|                0.62|\n",
            "| Kavya|   10-B|   92|           0.88|                0.81|\n",
            "| Rohit|   10-B|   85|            1.0|                0.85|\n",
            "| Sneha|   10-C|   80|           0.76|                0.61|\n",
            "+------+-------+-----+---------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module 6: Partitioned Data & Incremental Loading"
      ],
      "metadata": {
        "id": "yZsiLHudXflQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# full load\n",
        "df_final.write.partitionBy(\"section\").parquet(\"output/students/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "QGpWAMMqWS0q",
        "outputId": "fbb950df-a9ce-4794-dc04-688711be2972"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_ALREADY_EXISTS] Path file:/content/output/students already exists. Set mode as \"overwrite\" to overwrite the existing path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1380767245.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# full load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_final\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"section\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output/students/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/content/output/students already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Incremental Load\n",
        "incremental = [(\"Tejas\", \"10-A\", 91)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"section\", \"marks\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"section\").parquet(\"output/students/\")"
      ],
      "metadata": {
        "id": "V5dLQGzZXvQl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List files in output/students/ using Python\n",
        "\n",
        "!ls output/students/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-EGIggLYCxJ",
        "outputId": "6898b4b1-6cdf-4ed2-db63-f3dd6dadbb08"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'section=10-A'\t'section=10-B'\t'section=10-C'\t _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = spark.read.parquet('output/students/')\n",
        "df_all.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvo4R7j7YpPg",
        "outputId": "65c284b2-c2fb-4a79-cc8b-ba6b5df96b68"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+\n",
            "|  name|marks|section|\n",
            "+------+-----+-------+\n",
            "|Anjali|   78|   10-A|\n",
            "| Tejas|   91|   10-A|\n",
            "| Tejas|   91|   10-A|\n",
            "| Rohit|   85|   10-B|\n",
            "| Kavya|   92|   10-B|\n",
            "| Sneha|   80|   10-C|\n",
            "|  Amit|   89|   10-A|\n",
            "+------+-----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read only partition 10-A and list students\n",
        "df_10a = spark.read.parquet('output/students/section=10-A')\n",
        "df_10a.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxiwo_LuYIwl",
        "outputId": "3d493a7d-e92d-48b0-e7f6-97b609ba1d0c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+\n",
            "|  name|marks|\n",
            "+------+-----+\n",
            "|Anjali|   78|\n",
            "| Tejas|   91|\n",
            "| Tejas|   91|\n",
            "|  Amit|   89|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare before/after counts for section 10-A\n",
        "df_before = spark.read.parquet('output/students/')\n",
        "count_before = df_before.filter(\"section = '10-A'\").count()\n",
        "\n",
        "df_after = spark.read.parquet('output/students/')\n",
        "count_after = df_after.filter(\"section = '10-A'\").count()\n",
        "\n",
        "print(f\"Section 10-A count before: {count_before}\")\n",
        "print(f\"Section 10-A count after : {count_after}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVSVbuuWYO7h",
        "outputId": "64159771-727e-4b07-aafd-b779e04327b5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section 10-A count before: 4\n",
            "Section 10-A count after : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Module 7: ETL Pipeline - End to End"
      ],
      "metadata": {
        "id": "LSzc2vJ-YUL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# csv\n",
        "csv_data = \"\"\"\n",
        "emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,75000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,68000,4000\n",
        "4,Ramesh,Sales,58000,\n",
        "\"\"\"\n",
        "with open('infer.csv', 'w') as file:\n",
        "  file.write(csv_data)\n"
      ],
      "metadata": {
        "id": "f-RHYL6DYZbY"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating df\n",
        "df= spark.read.csv('infer.csv', header=True, inferSchema=True)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tf5uEZsvbow3",
        "outputId": "73b37097-f086-4576-c77a-282fc08052ec"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+------+\n",
            "|emp_id|  name|   dept|salary|bonus |\n",
            "+------+------+-------+------+------+\n",
            "|     1| Arjun|     IT| 75000| 5000 |\n",
            "|     2| Kavya|     HR| 62000|      |\n",
            "|     3| Sneha|Finance| 68000| 4000 |\n",
            "|     4|Ramesh|  Sales| 58000|  NULL|\n",
            "+------+------+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill null\n",
        "df_filled = df.fillna({\"bonus \": 2000})\n",
        "df_filled.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5ZAygPPawby",
        "outputId": "f37ec9df-7e93-4343-a4d6-641e9c6c5eb4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+------+\n",
            "|emp_id|  name|   dept|salary|bonus |\n",
            "+------+------+-------+------+------+\n",
            "|     1| Arjun|     IT| 75000| 5000 |\n",
            "|     2| Kavya|     HR| 62000|      |\n",
            "|     3| Sneha|Finance| 68000| 4000 |\n",
            "|     4|Ramesh|  Sales| 58000|  2000|\n",
            "+------+------+-------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TotalCTC\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_ctc = df_filled.withColumn(\"total_ctc\", col(\"salary\") + col(\"bonus \"))\n",
        "df_ctc.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I4bOJLTazGd",
        "outputId": "962fb8b8-a116-424d-d7b0-e2704e4d2475"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------+-------+------+------+---------+\n",
            "|emp_id|  name|   dept|salary|bonus |total_ctc|\n",
            "+------+------+-------+------+------+---------+\n",
            "|     1| Arjun|     IT| 75000| 5000 |  80000.0|\n",
            "|     2| Kavya|     HR| 62000|      |     NULL|\n",
            "|     3| Sneha|Finance| 68000| 4000 |  72000.0|\n",
            "|     4|Ramesh|  Sales| 58000|  2000|  60000.0|\n",
            "+------+------+-------+------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter employees with TotalCTC > 65000\n",
        "df_filtered = df_ctc.filter(col(\"total_ctc\") > 65000)\n",
        "df_filtered.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNPndFqya2Oh",
        "outputId": "23e204bb-2fbd-45c0-d461-0cdabc74e0aa"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+------+---------+\n",
            "|emp_id| name|   dept|salary|bonus |total_ctc|\n",
            "+------+-----+-------+------+------+---------+\n",
            "|     1|Arjun|     IT| 75000| 5000 |  80000.0|\n",
            "|     3|Sneha|Finance| 68000| 4000 |  72000.0|\n",
            "+------+-----+-------+------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save result in json\n",
        "df_filtered.write.mode(\"overwrite\").json(\"output/etl/employees_json\")\n"
      ],
      "metadata": {
        "id": "pCo2f8_Fa_Kl"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save result in parquet\n",
        "df_filtered.write.mode(\"overwrite\").partitionBy(\"dept\").parquet(\"output/etl/employees_parquet\")\n"
      ],
      "metadata": {
        "id": "c9CNJJAvbDRd"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZJu8t0lUcO4g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}