{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###1. PySpark Setup & Initialization"
      ],
      "metadata": {
        "id": "1DuY5yV4zYA7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "k0ODdzRMsP5I",
        "outputId": "7189f93e-761e-465a-a813-e0f0d86ca4f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7db022e951d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d7d88d049fc7:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>BotCampus Intermediate Session</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Creating SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"BotCampus Intermediate Session\") \\\n",
        ".master(\"local[*]\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Data and creating spark df\n",
        "data = [(\"Ananya\", \"Bangalore\", 24),\n",
        "(\"Ravi\", \"Hyderabad\", 28),\n",
        "(\"Kavya\", \"Delhi\", 22),\n",
        "(\"Meena\", \"Chennai\", 25)]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuDBdVz6zed0",
        "outputId": "6bcc15dc-b34e-44a6-a20f-ba651baab1c4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Ananya|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. RDDs & Transformations"
      ],
      "metadata": {
        "id": "kIUUxPfizxYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating RDD from feedback\n",
        "feedback = spark.sparkContext.parallelize([\n",
        "\"Ravi from Bangalore loved the mobile app\",\n",
        "\"Meena from Delhi reported poor response time\",\n",
        "\"Ajay from Pune liked the delivery speed\",\n",
        "\"Ananya from Hyderabad had an issue with UI\",\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])"
      ],
      "metadata": {
        "id": "a8pNBt_pzrKs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feedback.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24yXFteW0Hrz",
        "outputId": "0200bf34-10f5-402a-fac6-2198c50086d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ravi from Bangalore loved the mobile app',\n",
              " 'Meena from Delhi reported poor response time',\n",
              " 'Ajay from Pune liked the delivery speed',\n",
              " 'Ananya from Hyderabad had an issue with UI',\n",
              " 'Rohit from Mumbai gave positive feedback']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total number of words\n",
        "word_count = (feedback.flatMap(lambda feedback: feedback.split())\n",
        "              .map(lambda w: (w.lower(), 1))\n",
        "               .reduceByKey(lambda a,b: a+b))\n",
        "word_count.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72X1UHMT0KYH",
        "outputId": "9226870c-bb7c-4408-a94e-b6af08498010"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('from', 5),\n",
              " ('loved', 1),\n",
              " ('app', 1),\n",
              " ('poor', 1),\n",
              " ('response', 1),\n",
              " ('liked', 1),\n",
              " ('speed', 1),\n",
              " ('ananya', 1),\n",
              " ('an', 1),\n",
              " ('issue', 1),\n",
              " ('with', 1),\n",
              " ('rohit', 1),\n",
              " ('mumbai', 1),\n",
              " ('positive', 1),\n",
              " ('feedback', 1),\n",
              " ('ravi', 1),\n",
              " ('bangalore', 1),\n",
              " ('the', 2),\n",
              " ('mobile', 1),\n",
              " ('meena', 1),\n",
              " ('delhi', 1),\n",
              " ('reported', 1),\n",
              " ('time', 1),\n",
              " ('ajay', 1),\n",
              " ('pune', 1),\n",
              " ('delivery', 1),\n",
              " ('hyderabad', 1),\n",
              " ('had', 1),\n",
              " ('ui', 1),\n",
              " ('gave', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find top 3 most common words\n",
        "# takeOrdered() returns a list\n",
        "common_words = word_count.takeOrdered(3, key=lambda x: -x[1])\n",
        "print(common_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_OGrIXVVjjj",
        "outputId": "95bd7101-cab2-415f-9fb1-dc719aaafd90"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('from', 5), ('the', 2), ('loved', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stop Words\n",
        "# Creating a list of stop words\n",
        "stop_words = {\"from\", \"is\", \"with\", \"the\", \"a\",\"an\", \"of\", \"and\", \"on\", \"to\", \"in\" }\n",
        "no_stop_words = (feedback.flatMap(lambda line: line.split())\n",
        "                .map(lambda word : word.lower())\n",
        "                .filter(lambda word: word not in stop_words))\n",
        "no_stop_words.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-rwCRKXVoBJ",
        "outputId": "99781458-2ca1-4f82-f1a6-c6f829fe2c88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ravi',\n",
              " 'bangalore',\n",
              " 'loved',\n",
              " 'mobile',\n",
              " 'app',\n",
              " 'meena',\n",
              " 'delhi',\n",
              " 'reported',\n",
              " 'poor',\n",
              " 'response',\n",
              " 'time',\n",
              " 'ajay',\n",
              " 'pune',\n",
              " 'liked',\n",
              " 'delivery',\n",
              " 'speed',\n",
              " 'ananya',\n",
              " 'hyderabad',\n",
              " 'had',\n",
              " 'issue',\n",
              " 'ui',\n",
              " 'rohit',\n",
              " 'mumbai',\n",
              " 'gave',\n",
              " 'positive',\n",
              " 'feedback']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary of word -> count\n",
        "# collectAsMap() is used to create dictionary\n",
        "word_count.collectAsMap()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naIYCXUNVqiC",
        "outputId": "692f404a-4e7e-4efa-c847-cef1b9c43c68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'from': 5,\n",
              " 'loved': 1,\n",
              " 'app': 1,\n",
              " 'poor': 1,\n",
              " 'response': 1,\n",
              " 'liked': 1,\n",
              " 'speed': 1,\n",
              " 'ananya': 1,\n",
              " 'an': 1,\n",
              " 'issue': 1,\n",
              " 'with': 1,\n",
              " 'rohit': 1,\n",
              " 'mumbai': 1,\n",
              " 'positive': 1,\n",
              " 'feedback': 1,\n",
              " 'ravi': 1,\n",
              " 'bangalore': 1,\n",
              " 'the': 2,\n",
              " 'mobile': 1,\n",
              " 'meena': 1,\n",
              " 'delhi': 1,\n",
              " 'reported': 1,\n",
              " 'time': 1,\n",
              " 'ajay': 1,\n",
              " 'pune': 1,\n",
              " 'delivery': 1,\n",
              " 'hyderabad': 1,\n",
              " 'had': 1,\n",
              " 'ui': 1,\n",
              " 'gave': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. DataFrames - Transformations"
      ],
      "metadata": {
        "id": "MxUr3WDUdtWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating exam_scores DataFrame\n",
        "scores = [\n",
        "(\"Ravi\", \"Math\", 88),\n",
        "(\"Ananya\", \"Science\", 92),\n",
        "(\"Kavya\", \"English\", 79),\n",
        "(\"Ravi\", \"English\", 67),\n",
        "(\"Neha\", \"Math\", 94),\n",
        "(\"Meena\", \"Science\", 85)\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_scores = spark.createDataFrame(scores, columns)"
      ],
      "metadata": {
        "id": "DCxZElfJdmn5"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th3X3GqTfRzA",
        "outputId": "4167c896-0feb-4554-879d-51a8455500db"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+\n",
            "|  name|subject|score|\n",
            "+------+-------+-----+\n",
            "|  Ravi|   Math|   88|\n",
            "|Ananya|Science|   92|\n",
            "| Kavya|English|   79|\n",
            "|  Ravi|English|   67|\n",
            "|  Neha|   Math|   94|\n",
            "| Meena|Science|   85|\n",
            "+------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add grade column ( >=90 → A, 80-89 → B, 70-79 → C, elsve D)\n",
        "from pyspark.sql.functions import when, col, avg\n",
        "df_grade = df_scores.withColumn(\"Grade\",\n",
        "           when(col(\"score\") >= 90, \"A\")\n",
        "          .when((col(\"score\") >= 80) & (col(\"score\") < 90), \"B\")\n",
        "          .when((col(\"score\") >= 70) & (col(\"score\") < 80), \"C\")\n",
        "          .otherwise(\"D\"))\n",
        "df_grade.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrUgx4xIfbrc",
        "outputId": "0504b53f-c0de-49d3-d6d8-ae8aed578a64"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+\n",
            "|  name|subject|score|Grade|\n",
            "+------+-------+-----+-----+\n",
            "|  Ravi|   Math|   88|    B|\n",
            "|Ananya|Science|   92|    A|\n",
            "| Kavya|English|   79|    C|\n",
            "|  Ravi|English|   67|    D|\n",
            "|  Neha|   Math|   94|    A|\n",
            "| Meena|Science|   85|    B|\n",
            "+------+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by subject, find average score\n",
        "avg_score = df_scores.groupBy(\"subject\").agg(avg(\"score\").alias(\"avg_score\"))\n",
        "avg_score.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PutZRkTHjKce",
        "outputId": "b28ee5c5-7a0d-4b59-c3a5-f840a13b9309"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "|English|     73.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use when and otherwise to classify subject difficulty ( Math/Science =Difficult).\n",
        "difficult = df_scores.withColumn(\"Status\", when((col(\"subject\") == \"Math\") | (col(\"subject\") == \"Science\"), \"Difficult\")\n",
        "                                  .otherwise(\"Easy\"))\n",
        "difficult.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHwLC9iIjloz",
        "outputId": "9a079d7f-23d2-4895-cffe-c3911d89b643"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+---------+\n",
            "|  name|subject|score|   Status|\n",
            "+------+-------+-----+---------+\n",
            "|  Ravi|   Math|   88|Difficult|\n",
            "|Ananya|Science|   92|Difficult|\n",
            "| Kavya|English|   79|     Easy|\n",
            "|  Ravi|English|   67|     Easy|\n",
            "|  Neha|   Math|   94|Difficult|\n",
            "| Meena|Science|   85|Difficult|\n",
            "+------+-------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank students per subject using Window function.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "win = Window.partitionBy(\"subject\").orderBy(col(\"score\").desc())\n",
        "score_rank = df_scores.withColumn(\"rank\", rank().over(win))\n",
        "score_rank.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txAE00oajlGo",
        "outputId": "7f6162a7-3a83-4ea9-998e-22f145b26437"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+----+\n",
            "|  name|subject|score|rank|\n",
            "+------+-------+-----+----+\n",
            "| Kavya|English|   79|   1|\n",
            "|  Ravi|English|   67|   2|\n",
            "|  Neha|   Math|   94|   1|\n",
            "|  Ravi|   Math|   88|   2|\n",
            "|Ananya|Science|   92|   1|\n",
            "| Meena|Science|   85|   2|\n",
            "+------+-------+-----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply UDF to format names (e.g., make all uppercase)\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "# User defined function\n",
        "def u_name(name):\n",
        "  return name.upper()\n",
        "\n",
        "u_name_udf = udf(u_name, StringType())\n",
        "\n",
        "format_df = df_scores.withColumn(\"formatted_name\", u_name_udf(col(\"name\")))\n",
        "format_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mil6QO7NjzVR",
        "outputId": "5f4fcf1c-eeae-4330-997d-74ca9176dacb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+--------------+\n",
            "|  name|subject|score|formatted_name|\n",
            "+------+-------+-----+--------------+\n",
            "|  Ravi|   Math|   88|          RAVI|\n",
            "|Ananya|Science|   92|        ANANYA|\n",
            "| Kavya|English|   79|         KAVYA|\n",
            "|  Ravi|English|   67|          RAVI|\n",
            "|  Neha|   Math|   94|          NEHA|\n",
            "| Meena|Science|   85|         MEENA|\n",
            "+------+-------+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Ingest CSV & JSON = Save to Parquet"
      ],
      "metadata": {
        "id": "G2INdAIt2WMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 1\n",
        "data = \"\"\"\n",
        "id,name,department,city,salary\n",
        "1,Amit,IT,Bangalore,78000\n",
        "2,Kavya,HR,Chennai,62000\n",
        "3,Arjun,Finance,Hyderabad,55000\n",
        "\"\"\"\n",
        "with open('students.csv', 'w')as file:\n",
        "  file.write(data)"
      ],
      "metadata": {
        "id": "6-2ICR4g1BwJ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset 2\n",
        "import json\n",
        "data_2 = [\n",
        "  {\n",
        "    \"id\": 101,\n",
        "    \"name\": \"Sneha\",\n",
        "    \"address\": {\n",
        "    \"city\": \"Mumbai\",\n",
        "    \"pincode\": 400001\n",
        "  },\n",
        "    \"skills\": [\"Python\", \"Spark\"]\n",
        "  }\n",
        "]\n",
        "\n",
        "with open('employees_nested.json', 'w') as f:\n",
        "  json.dump( data_2, f, indent=4)"
      ],
      "metadata": {
        "id": "2rznLMxl26TT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load both datasets into pySpark\n",
        "students_df = spark.read.csv(\"students.csv\",header=True, inferSchema=True)\n",
        "students_df.show()\n",
        "\n",
        "employees_df = spark.read.option(\"multiline\", \"true\").json(\"employees_nested.json\")\n",
        "employees_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo2bYmXA4mD2",
        "outputId": "0dcd739e-0f3b-424f-87b5-b0d09eb1ad1f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+----------+---------+------+\n",
            "| id| name|department|     city|salary|\n",
            "+---+-----+----------+---------+------+\n",
            "|  1| Amit|        IT|Bangalore| 78000|\n",
            "|  2|Kavya|        HR|  Chennai| 62000|\n",
            "|  3|Arjun|   Finance|Hyderabad| 55000|\n",
            "+---+-----+----------+---------+------+\n",
            "\n",
            "+----------------+---+-----+---------------+\n",
            "|         address| id| name|         skills|\n",
            "+----------------+---+-----+---------------+\n",
            "|{Mumbai, 400001}|101|Sneha|[Python, Spark]|\n",
            "+----------------+---+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print schema and infer nested structure\n",
        "students_df.printSchema()\n",
        "employees_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dePe7gW14z2-",
        "outputId": "838566fa-2b4e-46c5-dd70-f4976ed5240c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- pincode: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the JSON (use explode , select , alias )\n",
        "from pyspark.sql.functions import explode\n",
        "\n",
        "flat_df = employees_df.select(col(\"id\"), col(\"name\"), col(\"address.city\").alias(\"city\"), col(\"address.pincode\").alias(\"pincode\"), explode(col(\"skills\")).alias(\"Skill\"))\n",
        "flat_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA8guiJS5QjX",
        "outputId": "67d3ed52-46f6-4608-fe9f-e186a4e333a8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-------+------+\n",
            "| id| name|  city|pincode| Skill|\n",
            "+---+-----+------+-------+------+\n",
            "|101|Sneha|Mumbai| 400001|Python|\n",
            "|101|Sneha|Mumbai| 400001| Spark|\n",
            "+---+-----+------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert both to Parquet and write to /tmp/output\n",
        "students_df.write.mode(\"overwrite\").parquet(\"/tmp/output/students_parquet\")\n",
        "employees_df.write.mode(\"overwrite\").parquet(\"/tmp/output/employees_parquet\")"
      ],
      "metadata": {
        "id": "Bwvzlm_65Uaf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading files\n",
        "!zip -r /tmp/output_parquet.zip /tmp/output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdqzXDxo9P5N",
        "outputId": "0965ff3f-55de-4741-e601-d0f946405932"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: tmp/output/ (stored 0%)\n",
            "  adding: tmp/output/students_parquet/ (stored 0%)\n",
            "  adding: tmp/output/students_parquet/._SUCCESS.crc (stored 0%)\n",
            "  adding: tmp/output/students_parquet/part-00000-64136b2b-7efd-4a40-a985-a0295683abb8-c000.snappy.parquet (deflated 48%)\n",
            "  adding: tmp/output/students_parquet/_SUCCESS (stored 0%)\n",
            "  adding: tmp/output/students_parquet/.part-00000-64136b2b-7efd-4a40-a985-a0295683abb8-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: tmp/output/employees_parquet/ (stored 0%)\n",
            "  adding: tmp/output/employees_parquet/._SUCCESS.crc (stored 0%)\n",
            "  adding: tmp/output/employees_parquet/.part-00000-a9c0a368-b850-4524-b349-708a7bee35e9-c000.snappy.parquet.crc (stored 0%)\n",
            "  adding: tmp/output/employees_parquet/_SUCCESS (stored 0%)\n",
            "  adding: tmp/output/employees_parquet/part-00000-a9c0a368-b850-4524-b349-708a7bee35e9-c000.snappy.parquet (deflated 53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/tmp/output_parquet.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9jsRUr1b8X2B",
        "outputId": "fda7d2d6-112a-4143-c2cc-94dcfa77907f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_93a001b2-db10-40fb-a4e6-78dc61dd8548\", \"output_parquet.zip\", 4271)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Spark SQL - Temp Views & Queries"
      ],
      "metadata": {
        "id": "Yl4jIJYc9dES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"Kavya\", \"English\", 79, \"B\"),\n",
        "    (\"Ravi\", \"English\", 67, \"C\"),\n",
        "    (\"Neha\", \"Math\", 94, \"A\"),\n",
        "    (\"Ravi\", \"Math\", 88, \"A\"),\n",
        "    (\"Ananya\", \"Science\", 92, \"A\"),\n",
        "    (\"Meena\", \"Science\", 85, \"B\"),\n",
        "    (\"Neha\", \"English\", 88, \"A\"),\n",
        "    (\"Ravi\", \"Science\", 91, \"A\"),\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\", \"grade\"]\n",
        "\n",
        "df_stuents = spark.createDataFrame(data, columns)\n",
        "df_stuents.createOrReplaceTempView(\"exam_scores\")"
      ],
      "metadata": {
        "id": "b0YQT6UB9Dpr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from exam_scores\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMocbxCO-uyD",
        "outputId": "e01f9dba-047f-4949-b58f-eaa1026f19b6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+\n",
            "|  name|subject|score|grade|\n",
            "+------+-------+-----+-----+\n",
            "| Kavya|English|   79|    B|\n",
            "|  Ravi|English|   67|    C|\n",
            "|  Neha|   Math|   94|    A|\n",
            "|  Ravi|   Math|   88|    A|\n",
            "|Ananya|Science|   92|    A|\n",
            "| Meena|Science|   85|    B|\n",
            "|  Neha|English|   88|    A|\n",
            "|  Ravi|Science|   91|    A|\n",
            "+------+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Top scorer per subject\n",
        "spark.sql(\"Select subject, name, score from(select *, RANK() OVER(PARTITION BY subject ORDER BY score DESC)as rnk from exam_scores)where rnk=1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjBawiLG-8Fl",
        "outputId": "710efef2-2dfe-4c7a-bd0c-5ff75be532c8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+-----+\n",
            "|subject|  name|score|\n",
            "+-------+------+-----+\n",
            "|English|  Neha|   88|\n",
            "|   Math|  Neha|   94|\n",
            "|Science|Ananya|   92|\n",
            "+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count of students per grade\n",
        "spark.sql(\"Select grade, count(*) as student_count from exam_scores group by grade\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvwia0RAANNV",
        "outputId": "e57aa903-f5d4-49ae-e887-d9a4d90f854b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+\n",
            "|grade|student_count|\n",
            "+-----+-------------+\n",
            "|    B|            2|\n",
            "|    C|            1|\n",
            "|    A|            5|\n",
            "+-----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Students with multiple subjects\n",
        "spark.sql(\"select name, count(DISTINCT subject)as subject_count from exam_scores group by name having subject_count >1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rb6GhUqAtij",
        "outputId": "6607805f-7625-493a-c829-441f6452fc0a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-------------+\n",
            "|name|subject_count|\n",
            "+----+-------------+\n",
            "|Ravi|            3|\n",
            "|Neha|            2|\n",
            "+----+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subjects with average score above 85\n",
        "spark.sql(\"select subject, avg(score)as AvgScore from exam_scores group by subject having AvgScore > 85\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS2-K5r_BIZ_",
        "outputId": "621faea0-1e20-4507-f01d-897c588f47c9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------+\n",
            "|subject|         AvgScore|\n",
            "+-------+-----------------+\n",
            "|   Math|             91.0|\n",
            "|Science|89.33333333333333|\n",
            "+-------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another DataFrame attendance(name, days_present) and:\n",
        "attendance_data = [\n",
        "    (\"Kavya\", 42),\n",
        "    (\"Ravi\", 35),\n",
        "    (\"Neha\", 48),\n",
        "    (\"Ananya\", 50),\n",
        "    (\"Meena\", 38)\n",
        "]\n",
        "\n",
        "attendance_columns = [\"name\", \"days_present\"]\n",
        "\n",
        "df_attendance = spark.createDataFrame(attendance_data, attendance_columns)\n",
        "df_attendance.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJf2ilVsBmvj",
        "outputId": "64d19b5e-4844-4637-a4cf-dc203ca808e7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+\n",
            "|  name|days_present|\n",
            "+------+------------+\n",
            "| Kavya|          42|\n",
            "|  Ravi|          35|\n",
            "|  Neha|          48|\n",
            "|Ananya|          50|\n",
            "| Meena|          38|\n",
            "+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Joining attendance df with students df\n",
        "df_join = df_stuents.join(df_attendance, on=\"name\", how=\"inner\")\n",
        "df_join.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2K-M8SKpB27V",
        "outputId": "e07c18e9-0cb5-4c54-86bf-c5bfa08cdf59"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+------------+\n",
            "|  name|subject|score|grade|days_present|\n",
            "+------+-------+-----+-----+------------+\n",
            "|Ananya|Science|   92|    A|          50|\n",
            "| Kavya|English|   79|    B|          42|\n",
            "| Meena|Science|   85|    B|          38|\n",
            "|  Neha|   Math|   94|    A|          48|\n",
            "|  Neha|English|   88|    A|          48|\n",
            "|  Ravi|English|   67|    C|          35|\n",
            "|  Ravi|   Math|   88|    A|          35|\n",
            "|  Ravi|Science|   91|    A|          35|\n",
            "+------+-------+-----+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating Attendance\n",
        "\n",
        "def downgrade(grade, days_present):\n",
        "    if days_present < 20:\n",
        "        downgrade_map = {\"A\": \"B\", \"B\": \"C\", \"C\": \"D\", \"D\": \"D\"}\n",
        "        return downgrade_map.get(grade, grade)\n",
        "    return grade\n",
        "\n",
        "downgrade_udf = udf(downgrade, StringType())\n",
        "\n",
        "df_adjusted = df_join.withColumn(\"adjusted_grade\", downgrade_udf(col(\"grade\"), col(\"days_present\")))\n",
        "\n",
        "df_adjusted.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5u4qSXwXCfP2",
        "outputId": "464d71ae-b706-40a6-c1dd-5489f5216893"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+------------+--------------+\n",
            "|  name|subject|score|grade|days_present|adjusted_grade|\n",
            "+------+-------+-----+-----+------------+--------------+\n",
            "|Ananya|Science|   92|    A|          50|             A|\n",
            "| Kavya|English|   79|    B|          42|             B|\n",
            "| Meena|Science|   85|    B|          38|             B|\n",
            "|  Neha|   Math|   94|    A|          48|             A|\n",
            "|  Neha|English|   88|    A|          48|             A|\n",
            "|  Ravi|English|   67|    C|          35|             C|\n",
            "|  Ravi|   Math|   88|    A|          35|             A|\n",
            "|  Ravi|Science|   91|    A|          35|             A|\n",
            "+------+-------+-----+-----+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Partitioned Load (Full + Incremental)"
      ],
      "metadata": {
        "id": "ba78Jtc6IEet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial load\n",
        "df_scores.write.partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "F6rTtgjoH3tl",
        "outputId": "64f90717-8d83-48ba-ed07-abb271348546"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_ALREADY_EXISTS] Path file:/tmp/scores already exists. Set mode as \"overwrite\" to overwrite the existing path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-406120800.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initial load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"subject\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/scores/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1721\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     def text(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/tmp/scores already exists. Set mode as \"overwrite\" to overwrite the existing path."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Incremental load\n",
        "incremental = [(\"Meena\", \"Math\", 93)]\n",
        "df_inc = spark.createDataFrame(incremental, [\"name\", \"subject\", \"score\"])\n",
        "df_inc.write.mode(\"append\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "aa4QkEyRINU2"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /tmp/scores/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFHsksX9IhPm",
        "outputId": "3db811f9-e4c3-49b2-eed5-64dcfe713ef3"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'subject=English'  'subject=Math'  'subject=Science'   _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_math = spark.read.parquet(\"/tmp/scores/subject=Math\")\n",
        "df_math.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctXvafDNJmyL",
        "outputId": "8aa16cfb-481e-4180-d0d9-456135b1337f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "| name|score|\n",
            "+-----+-----+\n",
            "|Meena|   93|\n",
            "|Meena|   93|\n",
            "| Neha|   94|\n",
            "| Ravi|   88|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ETL: Clean, Transform, Load"
      ],
      "metadata": {
        "id": "kmMZ2BSkJrti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw = \"\"\"\n",
        "emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,78000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,55000,3000\n",
        "\"\"\"\n",
        "with open ('etl.csv', 'w') as file:\n",
        "  file.write(raw)"
      ],
      "metadata": {
        "id": "-RBTyCcnJpbW"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data with header\n",
        "etl_df = spark.read.csv('etl.csv', header=True, inferSchema=True)\n",
        "etl_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pt7GSlcvK4js",
        "outputId": "1c348001-4ca1-4ec1-dfd8-aaed4026668e"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+\n",
            "|emp_id| name|   dept|salary|bonus|\n",
            "+------+-----+-------+------+-----+\n",
            "|     1|Arjun|     IT| 78000| 5000|\n",
            "|     2|Kavya|     HR| 62000| NULL|\n",
            "|     3|Sneha|Finance| 55000| 3000|\n",
            "+------+-----+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing bonus with 2000.\n",
        "filled_df = etl_df.fillna({'bonus': 2000})\n",
        "filled_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDZfDGE_LR_s",
        "outputId": "5e15a2d2-8e0f-4d8f-f5ec-57c96e110670"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+\n",
            "|emp_id| name|   dept|salary|bonus|\n",
            "+------+-----+-------+------+-----+\n",
            "|     1|Arjun|     IT| 78000| 5000|\n",
            "|     2|Kavya|     HR| 62000| 2000|\n",
            "|     3|Sneha|Finance| 55000| 3000|\n",
            "+------+-----+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total_ctc = salary + bonus\n",
        "\n",
        "df_ctc = filled_df.withColumn(\"TotalCTC\", col(\"salary\") + col(\"bonus\"))\n",
        "df_ctc.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVn1ERN8LjVl",
        "outputId": "571c2846-7b85-49ee-bb16-f4a8e3e13714"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+--------+\n",
            "|emp_id| name|   dept|salary|bonus|TotalCTC|\n",
            "+------+-----+-------+------+-----+--------+\n",
            "|     1|Arjun|     IT| 78000| 5000|   83000|\n",
            "|     2|Kavya|     HR| 62000| 2000|   64000|\n",
            "|     3|Sneha|Finance| 55000| 3000|   58000|\n",
            "+------+-----+-------+------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter where total_ctc > 60000\n",
        "filtered_df = df_ctc.filter(col(\"TotalCTC\") > 60000)\n",
        "filtered_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTBWUIbZL7cV",
        "outputId": "438dd42e-de10-4621-a122-7de99fd8abfa"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+----+------+-----+--------+\n",
            "|emp_id| name|dept|salary|bonus|TotalCTC|\n",
            "+------+-----+----+------+-----+--------+\n",
            "|     1|Arjun|  IT| 78000| 5000|   83000|\n",
            "|     2|Kavya|  HR| 62000| 2000|   64000|\n",
            "+------+-----+----+------+-----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to Parquet and JSON\n",
        "filtered_df.write.mode(\"overwrite\").parquet(\"/tmp/final_employees_parquet\")\n",
        "filtered_df.write.mode(\"overwrite\").json(\"/tmp/final_employees_json\")\n"
      ],
      "metadata": {
        "id": "BPYy_AHeML5Z"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "50dfMCFnMTdX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}